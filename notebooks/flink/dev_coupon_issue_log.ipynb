{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "autoscroll": "auto"
      },
      "source": [
        "# FlinkSQL Development to aggregate coupon issue log from MSK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "autoscroll": "auto"
      },
      "source": [
        "## 1. Source Table\n",
        "Connect to Kafka. Fill out your MSK broker url, group id and kafka topic name.\n",
        "* **properties.bootstrap.servers** : Comma separated list of Kafka brokers.\n",
        "* **properties.group.id** : The id of the consumer group for Kafka source. \n",
        "* **topic** : Topic name to read data from when the table is used as source.\n",
        " \n",
        "See also [Apache Kafka SQL Connector](https://nightlies.apache.org/flink/flink-docs-release-1.19/docs/connectors/table/kafka/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "%flink.ssql\n",
        "\n",
        "-- Source Table :: Coupon issue log\n",
        "\n",
        "CREATE TABLE source_table_dev (\n",
        "  msg_id STRING,\n",
        "  msg_type STRING,\n",
        "  device_id STRING,\n",
        "  location_code STRING,\n",
        "  coupon_code STRING,\n",
        "  response STRING,\n",
        "  create_time TIMESTAMP(3), \n",
        "  WATERMARK FOR create_time AS create_time - INTERVAL '5' SECOND \n",
        ") WITH (\n",
        "  'connector' = 'kafka',\n",
        "  'topic' = '<<YOUR-KAFKA-TOPIC-NAME>>',\n",
        "  'properties.bootstrap.servers' = '<<YOUR-KAFKA-BROKER-ENDPOINTS>>',\n",
        "  'properties.group.id' = '<<YOUR-CONSUMER-GROUP_ID>>',\n",
        "  'properties.security.protocol' = 'SASL_SSL', \n",
        "  'properties.sasl.mechanism' = 'AWS_MSK_IAM', \n",
        "  'properties.sasl.jaas.config' = 'software.amazon.msk.auth.iam.IAMLoginModule required;', \n",
        "  'properties.sasl.client.callback.handler.class' = 'software.amazon.msk.auth.iam.IAMClientCallbackHandler', \n",
        "  'format' = 'json', \n",
        "  'json.timestamp-format.standard' = 'ISO-8601', \n",
        "  'json.fail-on-missing-field' = 'false',\n",
        "  'json.ignore-parse-errors' = 'true', \n",
        "  'scan.startup.mode' = 'earliest-offset'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "autoscroll": "auto"
      },
      "source": [
        "## 2. Sink Table\n",
        "Put aggreagted features into Kafka topic as a sink. Fill out your MSK broker url and kafka topic name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "%flink.ssql\n",
        "\n",
        "-- (Query 1) Sink Table :: coupon-prefix-count\n",
        "\n",
        "CREATE TABLE sink_table_coupon_prefix_count_dev (\n",
        "  feature_group_name STRING,\n",
        "  loc_coupon_prefix STRING,\n",
        "  msg_count BIGINT,\n",
        "  event_time TIMESTAMP(3)\n",
        ") WITH (\n",
        "  'connector' = 'kafka',\n",
        "  'topic' = '<<YOUR-KAFKA-TOPIC-NAME>>',\n",
        "  'properties.bootstrap.servers' = '<<YOUR-KAFKA-BROKER-ENDPOINTS>>',\n",
        "  'properties.security.protocol' = 'SASL_SSL', \n",
        "  'properties.sasl.mechanism' = 'AWS_MSK_IAM', \n",
        "  'properties.sasl.jaas.config' = 'software.amazon.msk.auth.iam.IAMLoginModule required;', \n",
        "  'properties.sasl.client.callback.handler.class' = 'software.amazon.msk.auth.iam.IAMClientCallbackHandler', \n",
        "  'format' = 'json'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "%flink.ssql\n",
        "\n",
        "-- (Query 2) Sink Table :: proto-coupon-location-invalid-count\n",
        "\n",
        "CREATE TABLE sink_table_loc_invalid_cnt_dev (\n",
        "  feature_group_name STRING,\n",
        "  location_code STRING,\n",
        "  msg_count BIGINT,\n",
        "  event_time TIMESTAMP(3)\n",
        ") WITH (\n",
        "  'connector' = 'kafka',\n",
        "  'topic' = '<<YOUR-KAFKA-TOPIC-NAME>>',\n",
        "  'properties.bootstrap.servers' = '<<YOUR-KAFKA-BROKER-ENDPOINTS>>',\n",
        "  'properties.security.protocol' = 'SASL_SSL', \n",
        "  'properties.sasl.mechanism' = 'AWS_MSK_IAM', \n",
        "  'properties.sasl.jaas.config' = 'software.amazon.msk.auth.iam.IAMLoginModule required;', \n",
        "  'properties.sasl.client.callback.handler.class' = 'software.amazon.msk.auth.iam.IAMClientCallbackHandler', \n",
        "  'format' = 'json'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "autoscroll": "auto"
      },
      "source": [
        "## 3. Feature aggregation query\n",
        "Processing streamming data. \n",
        "* Modify feature_group_name to actual feature group name in Sageaker FeatureStore\n",
        "* In you want to push aggregated feature values, unlock 'Insert' code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "%flink.ssql\n",
        "\n",
        "-- (Query 1) coupon-prefix-count :: number of validation requests of last 10 minute, same coupon prefix from same location\n",
        "\n",
        "-- INSERT INTO sink_table_coupon_prefix_count \n",
        "SELECT \n",
        "  'proto-coupon-prefix-count' AS feature_group_name, \n",
        "  CONCAT(location_code,'#', SUBSTRING(coupon_code, 1, CHAR_LENGTH(coupon_code) - 4)) AS loc_coupon_prefix, \n",
        "  COUNT(*) AS msg_count, \n",
        "  MAX(create_time) AS event_time \n",
        "FROM source_table_dev \n",
        "GROUP BY \n",
        "  CONCAT(location_code,'#', SUBSTRING(coupon_code, 1, CHAR_LENGTH(coupon_code) - 4)), \n",
        "  HOP(create_time, INTERVAL '5' MINUTE, INTERVAL '10' MINUTES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto",
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "%flink.ssql\n",
        "\n",
        "-- (Query 2) proto-coupon-location-invalid-count :: invalid coupon reauest count in 5 minutes by location\n",
        "\n",
        "-- INSERT INTO sink_table_loc_invalid_cnt_dev \n",
        "SELECT \n",
        "  'proto-coupon-location-invalid-count' AS feature_group_name, \n",
        "  location_code AS location_code, \n",
        "  COUNT(*) AS msg_count, \n",
        "  MAX(create_time) AS event_time \n",
        "FROM source_table_dev \n",
        "WHERE response = 'INVALID' \n",
        "GROUP BY \n",
        "  location_code, \n",
        "  TUMBLE(create_time, INTERVAL '5' MINUTE)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "dev_coupon_issue_log"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
